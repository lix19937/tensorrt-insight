


https://en.wikipedia.org/wiki/Tesla_Autopilot_hardware


+ GPU VS NPU  

GPU从架构上算术逻辑单元的数量要远远多于Cache(缓存)和Control（控制器），由此导致其只适用于计算密集与数据并行的运算程序。 

所谓计算密集指：数值计算的比例要远大于内存操作，因此内存访问的延时可以被计算掩盖，对缓存的需求较低； 

数据并行则是说：大任务可以拆解为执行相同指令的小任务，因此对复杂流程控制的需求较低。 

AI机器学习正是这样的“运算程序”：将一个复杂问题拆解为众多的简单问题，一次性输入海量用于计算的参数。对简单问题解决的顺序性要求不高，最后输出整体结果即可。

NVIDIA的业务范畴数据中心  游戏  专业可视化  汽车  OEM和其他。

但云端（数据中心）和端侧（手机、智能汽车等移动端）场景中， AI芯片的运算方式有着本质性的差别。

首先，云端处理大批量一次性到达的累积数据（扩大批处理量，batch size），车端芯片则需要处理流数据，随着行驶（时间）陆续到来的数据；

第二，云端处理可以“等”数据“够了”再开始处理，车端则需要实时完成计算，尽可能得降低延迟，更勿论几秒钟的“等待”；

第三：在云端，任务本身是限定在虚拟世界，无需考虑与现实世界的交互。在车端则身处现实世界，每一个任务都需要考虑交互性。

此外，功耗和成本在车端AI芯片的考量中也占据更重的分量。

可见，云端AI芯片更侧重于数据吞吐量和支持多种AI任务的要求，车端的AI芯片则须保证很高的计算能效和实时性要求，能够实现端侧推断，以及低功耗、低延迟甚至低成本的要求。

但目前，英伟达端侧芯片的核心GPU架构仍是云端架构。 

CUDA核在每个GPU时钟中最多可以执行1个单精度乘法累加运算，适用于参数一致的AI模型深度学习以及高精度的高性能计算。
但对于AI模型来说，模型参数的权重各有不同，如果全部对标当中的高精度进行运算，则时间长且内存消耗大；而如果都降维到低精度参数，则输出的结果误差较大。 

张量核就可以做到混合精度：每1个GPU时钟执行1个矩阵乘法累加运算，输入矩阵是 FB16，乘法结果和累加器是FB32矩阵。
混合精度虽然在一定程度上牺牲了训练的精度，但可以减少内存的占用，以及缩短模型的训练时间。 

在扩充适应多样计算需求的算子同时，英伟达也在不断扩充算子所能支持的浮点精度。

CUDA核在最主流的FP32基础上，先后增加了对FP64、INT32 的计算能力；张量核则可支持FP16、INT8/INT4/Binary、TF32、 BF16、 FP64等多种数据精度的支持。 


+ 地平线  DSA（Domain Specific Architecture 特定领域架构）的芯片     


从物理世界来看，芯片架构就是在方寸之间（目前主流车规级量产芯片尺寸40nm-5nm）做文章：如何在有限的空间内排布算子、存储器以及之间的通信线路，不同的计算需求将导致不同的阵列方式。
